# Note: The 'pipeline' object (generator) is now passed into this function 
# from app.py where it is properly cached.

def generate_text(prompt, sentiment, max_length=150, generator=None):
    """
    Generates text using the provided GPT-2 pipeline, prepending a sentiment-aligned prefix.
    
    Args:
        prompt (str): The user's input text.
        sentiment (str): 'positive', 'negative', or 'neutral'.
        max_length (int): The maximum number of tokens for the generated text (prompt + new content).
        generator (transformers.pipeline): The cached GPT-2 pipeline object.
        
    Returns:
        str: The newly generated text, stripped of the original prompt and prefix.
    """
    if generator is None:
        # Fallback error for when the generator wasn't passed (should be caught in app.py)
        return "ERROR: Generator object not initialized."

    prefix_map = {
        "positive": "Here is an optimistic and cheerful take: ",
        "negative": "Here is a critical, cautious, or skeptical view: ",
        "neutral": "Here is a balanced and objective perspective: "
    }
    
    # Use the appropriate prefix, defaulting to neutral if somehow invalid
    sentiment_prefix = prefix_map.get(sentiment, prefix_map["neutral"])
    input_text = sentiment_prefix + prompt

    try:
        result = generator(
            input_text, 
            max_length=max_length, 
            num_return_sequences=1,
            # Added parameters to improve quality and variety of output
            do_sample=True, 
            top_k=50,       
            temperature=0.7 
        )
        
        full_text = result[0]['generated_text']
        
        # --- CRITICAL FIX: SLICING THE OUTPUT ---
        # Remove the input_text (prefix + prompt) from the start of the full_text
        if full_text.startswith(input_text):
            generated_only = full_text[len(input_text):].strip()
        else:
            # Fallback (in case of model tokenization quirks)
            generated_only = full_text 

        # Clean up any residual model-specific tokens
        generated_only = generated_only.replace("<|endoftext|>", "").strip()
        
        # Simple word count limiter (since GPT-2's max_length is tokens)
        # Split by space, take the first N tokens, and rejoin
        words = generated_only.split()
        # Estimate new tokens generated by subtracting a conservative estimate of the prompt tokens
        # We can't know exact token count, but this is a rough cutoff.
        
        return " ".join(words)

    except Exception as e:
        return f"An error occurred during text generation: {e}"
